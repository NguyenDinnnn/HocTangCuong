import numpy as np
import sys, os, pickle, random
from collections import defaultdict 
import math 

# Th√™m th∆∞ m·ª•c g·ªëc v√†o path ƒë·ªÉ import GridWorldEnv
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from app.robot_env import GridWorldEnv

# ---------------------------
# REWARD PARAMETERS (ƒê√É T·ªêI ∆ØU H√ìA: Ph·∫°t n·∫∑ng h∆°n ƒë·ªÉ ∆∞u ti√™n ƒë∆∞·ªùng ng·∫Øn)
# ---------------------------
REWARD_PARAMS = {
    "step_penalty": -1.0, 
    "wall_penalty": -50,         # TƒÉng m·∫°nh penalty
    "obstacle_penalty": -100,      # TƒÉng m·∫°nh penalty
    "waypoint_reward": 50,
    "goal_reward": 200,
    "goal_before_waypoints_penalty": -50,
}

# ---------------------------
# Env + RL params
# ---------------------------
# Kh·ªüi t·∫°o env c∆° b·∫£n
width, height = 10, 8
env = GridWorldEnv(width=width, height=height, start=(0,0), goal=(9,7), 
                   obstacles=[(1,1),(2,3),(4,4),(5,1),(7,6)], 
                   waypoints=[(3,2),(6,5)], 
                   max_steps=500) 

# √Åp d·ª•ng tham s·ªë Reward t·ªëi ∆∞u v√†o m√¥i tr∆∞·ªùng
for k, v in REWARD_PARAMS.items():
    setattr(env, k, v)

# Q-Learning parameters
actions = ['up', 'right', 'down', 'left']
gamma = 0.995 # << T·ªêI ∆ØU H√ìA: TƒÉng chi·∫øt kh·∫•u ƒë·ªÉ ∆∞u ti√™n ph·∫ßn th∆∞·ªüng d√†i h·∫°n
alpha = 0.1
epsilon = 1.0
epsilon_min = 0.01 
epsilon_decay = 0.998 # ƒêi·ªÅu ch·ªânh nh·∫π ƒë·ªÉ k√©o d√†i th·ªùi gian Exploration h∆°n, ph√π h·ª£p v·ªõi gamma cao
episodes = 50000 # TƒÉng s·ªë l∆∞·ª£ng episodes ƒë·ªÉ ƒë·∫£m b·∫£o h·ªôi t·ª• v·ªõi gamma cao h∆°n

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "models"))
os.makedirs(BASE_DIR, exist_ok=True)
q_path = os.path.join(BASE_DIR, "qlearning_qtable_abstract_optimized.pkl") # T√™n file m·ªõi

def _new_qrow():
    return {a: 0.0 for a in actions}

def encode_visited(wp_list, visited_set):
    """M√£ h√≥a tr·∫°ng th√°i Waypoint ƒë√£ thƒÉm d∆∞·ªõi d·∫°ng m·ªôt s·ªë nguy√™n."""
    code = 0
    for i, wp in enumerate(wp_list):
        if wp in visited_set:
            code |= (1 << i)
    return code

# >> H√ÄM T·ªêI ∆ØU H√ìA TR·∫†NG TH√ÅI TR·ª™U T∆Ø·ª¢NG (Enhanced State Abstraction)
def get_abstract_state(env):
    """
    Tr·∫£ v·ªÅ tr·∫°ng th√°i tr·ª´u t∆∞·ª£ng h√≥a (relative direction + local obstacles).
    Keys: (sign_dx_goal, sign_dy_goal, sign_dx_nearest_wp, sign_dy_nearest_wp, 
            visited_code, local_obstacle_code)
    """
    robot_x, robot_y = env.get_state()
    goal_x, goal_y = env.goal
    
    # H√†m Sign: L·∫•y h∆∞·ªõng t∆∞∆°ng ƒë·ªëi
    def get_sign(delta):
        if delta > 0: return 1
        if delta < 0: return -1
        return 0
    
    # 1. Delta t·ªõi Goal (d∆∞·ªõi d·∫°ng Sign)
    delta_x_goal = goal_x - robot_x
    delta_y_goal = goal_y - robot_y
    sign_dx_goal = get_sign(delta_x_goal)
    sign_dy_goal = get_sign(delta_y_goal)
    
    # 2. Delta t·ªõi Waypoint g·∫ßn nh·∫•t (ch∆∞a thƒÉm)
    targets = [wp for wp in env.waypoints if wp not in env.visited_waypoints]
    
    sign_dx_nearest_wp = 0
    sign_dy_nearest_wp = 0
    
    if targets:
        # T√¨m Waypoint g·∫ßn nh·∫•t (d√πng kho·∫£ng c√°ch Manhattan)
        def manhattan_distance(wp):
            return abs(wp[0]-robot_x) + abs(wp[1]-robot_y)
            
        nearest_wp = min(targets, key=manhattan_distance)
        
        delta_x_nearest_wp = nearest_wp[0] - robot_x
        delta_y_nearest_wp = nearest_wp[1] - robot_y
        
        sign_dx_nearest_wp = get_sign(delta_x_nearest_wp)
        sign_dy_nearest_wp = get_sign(delta_y_nearest_wp)
    
    # 3. Visited Code
    visited_code = encode_visited(env.waypoints, env.visited_waypoints)
    
    # T·ªêI ∆ØU H√ìA 2: Local Obstacle Code (4-bit code)
    # L√™n (0b0001), Ph·∫£i (0b0010), Xu·ªëng (0b0100), Tr√°i (0b1000)
    local_obstacle_code = 0
    
    # V·ªã tr√≠ c√°c √¥ l√¢n c·∫≠n t∆∞∆°ng ·ª©ng v·ªõi action: up, right, down, left
    neighbors = {
        'up': (robot_x, robot_y - 1),
        'right': (robot_x + 1, robot_y),
        'down': (robot_x, robot_y + 1),
        'left': (robot_x - 1, robot_y),
    }

    # Ki·ªÉm tra xem √¥ l√¢n c·∫≠n c√≥ ph·∫£i l√† ch∆∞·ªõng ng·∫°i v·∫≠t hay t∆∞·ªùng kh√¥ng
    if neighbors['up'] in env.obstacles or not (0 <= neighbors['up'][1] < env.height):
        local_obstacle_code |= 0b0001
    if neighbors['right'] in env.obstacles or not (0 <= neighbors['right'][0] < env.width):
        local_obstacle_code |= 0b0010
    if neighbors['down'] in env.obstacles or not (0 <= neighbors['down'][1] < env.height):
        local_obstacle_code |= 0b0100
    if neighbors['left'] in env.obstacles or not (0 <= neighbors['left'][0] < env.width):
        local_obstacle_code |= 0b1000
    
    # K·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng t∆∞∆°ng ƒë·ªëi th√†nh Full Abstract State Key
    return (sign_dx_goal, sign_dy_goal, 
            sign_dx_nearest_wp, sign_dy_nearest_wp, 
            visited_code, local_obstacle_code)

# >> H√ÄM NG·∫™U NHI√äN H√ìA M√îI TR∆Ø·ªúNG (Domain Randomization)
def randomize_environment(env_instance, max_obstacles=10, max_waypoints=3):
    """
    Thi·∫øt l·∫≠p ng·∫´u nhi√™n s·ªë l∆∞·ª£ng v√† v·ªã tr√≠ Goal, Waypoints v√† Obstacles cho m√¥i tr∆∞·ªùng.
    Gi·ªØ nguy√™n logic randomization.
    """
    w, h = env_instance.width, env_instance.height
    start = env_instance.start 
    
    all_cells = [(x, y) for x in range(w) for y in range(h) if (x, y) != start]
    random.shuffle(all_cells)
    
    # Ng·∫´u nhi√™n h√≥a s·ªë l∆∞·ª£ng Waypoints v√† Obstacles
    num_waypoints = random.randint(1, max_waypoints) 
    
    min_obs = 5 # ƒê·∫£m b·∫£o c√≥ m·ªôt l∆∞·ª£ng ch∆∞·ªõng ng·∫°i v·∫≠t nh·∫•t ƒë·ªãnh
    max_cells_for_obs = len(all_cells) - num_waypoints - 1 
    
    num_obstacles = random.randint(min(min_obs, max_cells_for_obs), min(max_obstacles, max_cells_for_obs))
    
    num_reserved = num_waypoints + 1 # 1 cho goal, num_waypoints cho waypoints
    
    # 1. Random Obstacles
    # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° s·ªë √¥ c√≤n l·∫°i sau khi tr·ª´ ƒëi start v√† c√°c √¥ reserved
    if len(all_cells) - num_reserved < num_obstacles: 
        num_obstacles = len(all_cells) - num_reserved
    
    obstacles = all_cells[:num_obstacles]
    
    # 2. Random Waypoints v√† Goal
    remain = [cell for cell in all_cells if cell not in obstacles]
    
    # ƒê·∫£m b·∫£o c√≥ ƒë·ªß √¥ tr·ªëng cho Waypoints v√† Goal
    if len(remain) < num_reserved:
        needed = num_reserved - len(remain)
        if needed > 0:
            remain.extend(obstacles[-needed:]) # L·∫•y l·∫°i b·ªõt obstacle n·∫øu thi·∫øu
            obstacles = obstacles[:-needed]
    
    # Ch·ªçn ng·∫´u nhi√™n Waypoints v√† Goal t·ª´ c√°c √¥ c√≤n l·∫°i
    if len(remain) >= num_reserved:
        chosen_coords = random.sample(remain, num_reserved)
    else:
        # D√πng to√†n b·ªô √¥ c√≤n l·∫°i n·∫øu kh√¥ng ƒë·ªß (tr∆∞·ªùng h·ª£p c·ª±c hi·∫øm)
        chosen_coords = remain
        num_waypoints = len(chosen_coords) - 1 # ƒêi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng waypoint
    
    if num_waypoints > 0:
        waypoints = chosen_coords[:-1]
        goal = chosen_coords[-1]
    elif chosen_coords: # Ch·ªâ c√≥ goal
        waypoints = []
        goal = chosen_coords[0]
    else: # Tr∆∞·ªùng h·ª£p l·ªói kh√¥ng c√≤n √¥ n√†o
        goal = (w-1, h-1)
        waypoints = []
        obstacles = []


    # C·∫≠p nh·∫≠t m√¥i tr∆∞·ªùng
    env_instance.obstacles = set(obstacles)
    env_instance.waypoints = waypoints
    env_instance.goal = goal


def _load_qtable():
    if os.path.exists(q_path) and os.path.getsize(q_path) > 0:
        try:
            with open(q_path, "rb") as f:
                loaded_data = pickle.load(f)
            if isinstance(loaded_data, dict) and 'Q' in loaded_data:
                return loaded_data['Q'], loaded_data.get('epsilon', epsilon)
            else:
                return loaded_data, epsilon
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng load ƒë∆∞·ª£c {q_path}: {e}. D√πng b·∫£ng r·ªóng.")
    return {}, epsilon

# Q l√† defaultdict ƒë·ªÉ t·ª± ƒë·ªông kh·ªüi t·∫°o Q-value cho tr·∫°ng th√°i m·ªõi
Q, current_epsilon = _load_qtable()
Q = defaultdict(_new_qrow, Q)
print(f"‚úÖ Loaded Q-learning Q-table: {len(Q)} states. Epsilon: {current_epsilon:.4f}")


def choose_action(abstract_state, current_epsilon):
    """Ch·ªçn h√†nh ƒë·ªông theo chi·∫øn l∆∞·ª£c epsilon-greedy, s·ª≠ d·ª•ng Abstract State."""
    if abstract_state not in Q: 
        Q[abstract_state] = _new_qrow()
    
    if random.random() < current_epsilon:
        return random.choice(actions) 
    else:
        q_values = Q[abstract_state] 
        max_q = max(q_values.values())
        best_actions = [a for a, q in q_values.items() if q == max_q]
        return random.choice(best_actions)

for ep in range(episodes):
    # I. B∆Ø·ªöC QUAN TR·ªåNG: NG·∫™U NHI√äN H√ìA M√îI TR∆Ø·ªúNG
    randomize_environment(env, max_obstacles=10, max_waypoints=3) 
    
    # Reset env: TƒÉng Max Steps ƒë·ªÉ robot kh√¥ng b·ªã time-out trong m√¥i tr∆∞·ªùng ph·ª©c t·∫°p
    env.reset(max_steps=500) 
    
    abstract_state = get_abstract_state(env) # D√πng h√†m Sign-based T·ªêI ∆ØU

    done = False
    episode_reward = 0
    
    while not done:
        # Ch·ªçn action theo abstract_state
        action = choose_action(abstract_state, current_epsilon)
        action_idx = actions.index(action)

        old_abstract_state = abstract_state 
        
        # Th·ª±c hi·ªán action
        _, reward, done_raw, _ = env.step(action_idx)
        
        # T·∫°o next_abstract_state
        next_abstract_state = get_abstract_state(env) 

        episode_reward += reward

        # K√≠ch ho·∫°t done khi ƒë·∫°t m·ª•c ti√™u ho·∫∑c h·∫øt max_steps
        done = env.is_done() or env.steps >= env.max_steps
        
        # Standard Q-learning update
        max_next_Q = 0.0
        if not done:
            # L·∫•y Q-value l·ªõn nh·∫•t c·ªßa tr·∫°ng th√°i ti·∫øp theo
            max_next_Q = max(Q[next_abstract_state].values())
        
        target = reward + gamma * max_next_Q
        
        # C·∫≠p nh·∫≠t Q-value
        Q[old_abstract_state][action] += alpha * (target - Q[old_abstract_state][action])

        abstract_state = next_abstract_state
    
    # 3. Gi·∫£m epsilon
    current_epsilon = max(epsilon_min, current_epsilon * epsilon_decay)
    
    if (ep + 1) % 100 == 0:
        print(f"Episode {ep + 1}/{episodes} | Epsilon: {current_epsilon:.4f} | Steps: {env.steps} | Total Reward: {episode_reward:.2f} | States: {len(Q)}")

    if (ep + 1) % 500 == 0:
        # L∆∞u Q-table v√† epsilon ƒë·ªãnh k·ª≥
        with open(q_path, "wb") as f:
            pickle.dump({'Q': dict(Q), 'epsilon': current_epsilon}, f)
        print(f"üíæ Saved checkpoint at episode {ep + 1}.")

# L∆∞u Q-table cu·ªëi c√πng
with open(q_path, "wb") as f:
    pickle.dump({'Q': dict(Q), 'epsilon': current_epsilon}, f)
print(f"\n‚úÖ Training complete. Final Q-table saved to {q_path}.")
