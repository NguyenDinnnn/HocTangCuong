import sys, os, pickle
import numpy as np
from collections import defaultdict
from datetime import datetime
import random 
import math 
from typing import Tuple, List, Set, Optional

# Gi·∫£ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n import ƒë√£ ƒë√∫ng
try:
    # C·∫ßn ƒë·∫£m b·∫£o c·∫•u tr√∫c th∆∞ m·ª•c ƒë√∫ng: clients/models/train_qlearning...py -> ../app/robot_env.py
    # Th√™m th∆∞ m·ª•c g·ªëc v√†o path ƒë·ªÉ import GridWorldEnv
    # S·ª≠ d·ª•ng os.path.join ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t∆∞∆°ng th√≠ch v·ªõi m·ªçi h·ªá ƒëi·ªÅu h√†nh
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
    from app.robot_env import GridWorldEnv
except ImportError:
    print("WARNING: Kh√¥ng th·ªÉ import GridWorldEnv. Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n.")
    
    # --------------------------------------------------------------------------
    # L·ªõp gi·∫£ l·∫≠p GridWorldEnv ƒë·ªÉ tr·∫£ v·ªÅ ƒë√∫ng ki·ªÉu d·ªØ li·ªáu
    # --------------------------------------------------------------------------
    class GridWorldEnv: 
        ACTION_NAMES = ["up", "right", "down", "left"]
        ACTIONS = [(0, -1), (1, 0), (0, 1), (-1, 0)]
        def __init__(self, **kwargs): pass
        def reset(self, **kwargs): 
            self.state = (0, 0)
            self.steps = 0
            self.visited_waypoints = set()
            self.max_steps = kwargs.get('max_steps', 500)
            return self.state
        def get_state(self): return self.state
        def is_done(self): return self.steps >= self.max_steps
        def step(self, action_idx): 
            self.steps += 1
            # Gi·∫£ l·∫≠p reward v√† done
            return (0, 0), -0.5, self.steps >= self.max_steps, {}
        
        # S·ª≠a l·∫°i __getattr__ ƒë·ªÉ tr·∫£ v·ªÅ c√°c gi√° tr·ªã/ki·ªÉu d·ªØ li·ªáu m·∫∑c ƒë·ªãnh an to√†n
        def __getattr__(self, name):
            if name in ['width', 'height', 'steps', 'max_steps']: return 10 if name in ['width', 'height'] else 0
            if name in ['state', 'goal']: return (0,0) 
            if name in ['waypoints', 'visited_waypoints', 'obstacles']: 
                return set() if name == 'visited_waypoints' or name == 'obstacles' else []
            if name.endswith('penalty') or name.endswith('reward'): return 0 
            return super().__getattr__(name)
    
    # G√°n c√°c thu·ªôc t√≠nh c∆° b·∫£n cho l·ªõp gi·∫£ l·∫≠p 
    GridWorldEnv.width = 10
    GridWorldEnv.height = 8
    GridWorldEnv.state = (0, 0)
    GridWorldEnv.goal = (9, 7)
    GridWorldEnv.waypoints = [(3, 2), (6, 5)]
    GridWorldEnv.visited_waypoints = set()
    GridWorldEnv.obstacles = set()
    GridWorldEnv.steps = 0
    GridWorldEnv.max_steps = 500
    GridWorldEnv.step_penalty = -0.5
    
    # --------------------------------------------------------------------------


# ==============================================================================
# STATE ABSTRACTION FUNCTIONS (GI·ªÆ NGUY√äN)
# ==============================================================================

def encode_visited(wp_list: List[Tuple[int, int]], visited_set: Set[Tuple[int, int]]) -> int:
    """M√£ h√≥a tr·∫°ng th√°i Waypoint ƒë√£ thƒÉm d∆∞·ªõi d·∫°ng m·ªôt s·ªë nguy√™n (bitmask)."""
    code = 0
    for i, wp in enumerate(wp_list): 
        if wp in visited_set:
            code |= (1 << i)
    return code

def get_abstract_state(env: GridWorldEnv) -> Tuple:
    """
    T·∫°o tr·∫°ng th√°i tr·ª´u t∆∞·ª£ng (State Abstraction) t·ª´ m√¥i tr∆∞·ªùng.
    Tr·∫°ng th√°i tr·ª´u t∆∞·ª£ng: (Goal_Sign_X, Goal_Sign_Y, Waypoint_Sign_X, Waypoint_Sign_Y, Obstacle_Code, Visited_Code)
    """
    rx, ry = env.state
    
    # 1. T√≠n hi·ªáu Goal (Relative Goal Sign)
    gx, gy = env.goal
    dx_goal = gx - rx
    dy_goal = gy - ry
    sign_x_goal = int(math.copysign(1, dx_goal)) if dx_goal != 0 else 0
    sign_y_goal = int(math.copysign(1, dy_goal)) if dy_goal != 0 else 0

    # 2. T√≠n hi·ªáu Waypoint g·∫ßn nh·∫•t ch∆∞a thƒÉm (Relative Waypoint Sign)
    closest_wp = None
    min_dist = float('inf')
    
    for wp in env.waypoints:
        if wp not in env.visited_waypoints:
            dist = abs(wp[0] - rx) + abs(wp[1] - ry)
            if dist < min_dist:
                min_dist = dist
                closest_wp = wp
    
    sign_x_wp, sign_y_wp = 0, 0
    if closest_wp:
        dx_wp = closest_wp[0] - rx
        dy_wp = closest_wp[1] - ry
        sign_x_wp = int(math.copysign(1, dx_wp)) if dx_wp != 0 else 0
        sign_y_wp = int(math.copysign(1, dy_wp)) if dy_wp != 0 else 0

    # 3. M√£ ch∆∞·ªõng ng·∫°i v·∫≠t c·ª•c b·ªô (Local Obstacle Code)
    obstacle_code = 0
    for i, (dx, dy) in enumerate(GridWorldEnv.ACTIONS):
        next_pos = (rx + dx, ry + dy)
        # Ki·ªÉm tra t∆∞·ªùng ho·∫∑c ch∆∞·ªõng ng·∫°i v·∫≠t
        if next_pos in env.obstacles or not (0 <= next_pos[0] < env.width and 0 <= next_pos[1] < env.height):
            obstacle_code |= (1 << i)
            
    # 4. M√£ Waypoint ƒë√£ thƒÉm (Visited Code)
    visited_code = encode_visited(env.waypoints, env.visited_waypoints)

    return (sign_x_goal, sign_y_goal, sign_x_wp, sign_y_wp, obstacle_code, visited_code)

# ==============================================================================
# H√ÄM NG·∫™U NHI√äN H√ìA M√îI TR∆Ø·ªúNG (CHO M√îI TR∆Ø·ªúNG ƒê·ªòNG)
# ==============================================================================

def randomize_env_params(width: int, height: int, num_waypoints: int = 2, min_obs: int = 5, max_obs: int = 10) -> Tuple[Tuple[int, int], Tuple[int, int], List[Tuple[int, int]], Set[Tuple[int, int]]]:
    """T·∫°o ng·∫´u nhi√™n v·ªã tr√≠ b·∫Øt ƒë·∫ßu, ƒë√≠ch, waypoint v√† ch∆∞·ªõng ng·∫°i v·∫≠t kh√¥ng tr√πng l·∫∑p."""
    all_coords = set((x, y) for x in range(width) for y in range(height))
    
    required_num = 2 + num_waypoints 
    
    if len(all_coords) < required_num:
        raise ValueError("L∆∞·ªõi qu√° nh·ªè ƒë·ªÉ ch·ªçn ƒë·ªß v·ªã tr√≠ b·∫Øt ƒë·∫ßu, ƒë√≠ch, v√† waypoint.")
        
    required_positions = random.sample(list(all_coords), required_num)
    
    start = required_positions[0]
    goal = required_positions[1]
    waypoints = required_positions[2:]
    
    available_coords = all_coords - set(required_positions)
    
    num_obstacles = random.randint(min_obs, min(max_obs, len(available_coords)))
    obstacles = set(random.sample(list(available_coords), num_obstacles))

    return start, goal, waypoints, obstacles


# ==============================================================================
# 1. THAM S·ªê RL V√Ä KH·ªûI T·∫†O (ƒê√É T·ªêI ∆ØU H√ìA)
# ==============================================================================

EPISODES = 100000
INITIAL_EPSILON = 1.0
MIN_EPSILON = 0.01  
ALPHA = 0.1 

# T·ªêI ∆ØU: TƒÉng Gamma ƒë·ªÉ agent si√™u xa th·ªã (Super long-term vision)
GAMMA = 0.995

# T·ªêI ∆ØU: Gi·∫£m Epsilon decay rate ƒë·ªÉ tƒÉng th·ªùi gian kh√°m ph√°
EPSILON_DECAY_RATE = 0.998 

# Tham s·ªë m√¥i tr∆∞·ªùng
WIDTH, HEIGHT = 10, 8
MAX_STEPS = 500

# REWARD T·ªêI ∆ØU cho ƒê∆Ø·ªúNG ƒêI NG·∫ÆN NH·∫§T & TH·ª® T·ª∞ WAYPOINT
REWARDS = {
    # T·ªêI ∆ØU: TƒÉng ph·∫°t b∆∞·ªõc ƒëi ƒë·ªÉ bu·ªôc t√¨m ƒë∆∞·ªùng ng·∫Øn (Shortest path enforcement)
    "step_penalty": -1.0, 
    "wall_penalty": -20,
    "obstacle_penalty": -50, 
    # T·ªêI ∆ØU: TƒÉng th∆∞·ªüng Waypoint ƒë·ªÉ c√¢n b·∫±ng v·ªõi Goal Reward (Waypoint Priority)
    "waypoint_reward": 100, 
    "goal_reward": 200,
    "goal_before_waypoints_penalty": -50, 
}

# Kh·ªüi t·∫°o M√¥i tr∆∞·ªùng v·ªõi c√°c tham s·ªë C·ªê ƒê·ªäNH BAN ƒê·∫¶U
START, GOAL = (0,0), (9,7)
WAYPOINTS = [(3,2),(6,5)]
OBSTACLES = [(1,1),(2,3),(4,4),(5,1),(7,6)]

env = GridWorldEnv(width=WIDTH, height=HEIGHT, start=START, goal=GOAL, 
                   obstacles=OBSTACLES, waypoints=WAYPOINTS, max_steps=MAX_STEPS)

# √Åp d·ª•ng c√°c tham s·ªë Reward ƒë√£ t·ªëi ∆∞u v√†o m√¥i tr∆∞·ªùng
for k, v in REWARDS.items():
    if hasattr(env, k):
        setattr(env, k, v)
        
actions = env.ACTION_NAMES
# Q-table TD (Q-learning) s·ª≠ d·ª•ng State Abstraction
Q = defaultdict(lambda: {a: 0.0 for a in actions}) 

# ƒê∆∞·ªùng d·∫´n l∆∞u model
model_dir = os.path.join(os.path.dirname(__file__), "clients/models") 
os.makedirs(model_dir, exist_ok=True)
q_file = os.path.join(model_dir, "mc_qtable.pkl")

# Load model (n·∫øu c√≥)
start_episode = 0
epsilon = INITIAL_EPSILON
if os.path.exists(q_file):
    try:
        with open(q_file, "rb") as f:
            data = pickle.load(f)
        
        Q.update(data['Q'])
        epsilon = data.get('epsilon', INITIAL_EPSILON)
        start_episode = data.get('episode', 0) + 1
        print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh Q-learning (Abstraction) t·ª´ episode {start_episode - 1}. Epsilon: {epsilon:.6f}")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh: {e}. B·∫Øt ƒë·∫ßu train l·∫°i t·ª´ ƒë·∫ßu.")

# ==============================================================================
# 2. H√ÄM CH·ªåN H√ÄNH ƒê·ªòNG V√Ä HU·∫§N LUY·ªÜN
# ==============================================================================
def choose_action(state_tuple, current_epsilon):
    """L·ª±a ch·ªçn h√†nh ƒë·ªông theo ch√≠nh s√°ch Epsilon-Greedy."""
    if np.random.rand() < current_epsilon:
        # Exploration: Ch·ªçn ng·∫´u nhi√™n
        return np.random.choice(actions) 
    else:
        # Exploitation: Ch·ªçn h√†nh ƒë·ªông c√≥ Q-value l·ªõn nh·∫•t
        q_values = Q[state_tuple]
        if all(q == 0.0 for q in q_values.values()):
             # N·∫øu t·∫•t c·∫£ ƒë·ªÅu b·∫±ng 0, ch·ªçn ng·∫´u nhi√™n
             return np.random.choice(actions)
            
        max_q = max(q_values.values())
        # Ch·ªçn ng·∫´u nhi√™n trong s·ªë c√°c h√†nh ƒë·ªông t·ªët nh·∫•t (break ties)
        best_actions = [a for a, q in q_values.items() if q == max_q]
        return np.random.choice(best_actions)

def train_qlearning_with_abstraction(): 
    global epsilon
    start_time = datetime.now()
    
    success_count = 0
    
    for episode in range(start_episode, EPISODES):
        
        # --- B∆Ø·ªöC 1: NG·∫™U NHI√äN H√ìA M√îI TR∆Ø·ªúNG ---
        try:
            start, goal, waypoints, obstacles = randomize_env_params(
                WIDTH, HEIGHT, num_waypoints=2, min_obs=5, max_obs=10
            )
        except ValueError as e:
            continue 
        
        # Reset m√¥i tr∆∞·ªùng v·ªõi tham s·ªë ng·∫´u nhi√™n
        env.reset(start=start, goal=goal, obstacles=obstacles, waypoints=waypoints) 
        
        # L·∫•y tr·∫°ng th√°i tr·ª´u t∆∞·ª£ng ban ƒë·∫ßu
        abstract_state = get_abstract_state(env) 
        done = False
        total_reward = 0
        
        # --- B·∫ÆT ƒê·∫¶U EPISODE ---
        while not done:
            # 1. Ch·ªçn H√†nh ƒë·ªông
            action_name = choose_action(abstract_state, epsilon)
            
            old_abstract_state = abstract_state
            
            # 2. Th·ª±c hi·ªán H√†nh ƒë·ªông
            action_idx = env.ACTION_NAMES.index(action_name) 
            _, reward, _, _ = env.step(action_idx)
            
            # 3. L·∫•y Tr·∫°ng th√°i M·ªöI
            next_abstract_state = get_abstract_state(env)
            
            # K√≠ch ho·∫°t done khi ƒë·∫°t m·ª•c ti√™u ho·∫∑c h·∫øt max_steps
            done = env.is_done() or env.steps >= env.max_steps
            
            # 4. C·∫¨P NH·∫¨T Q-VALUE (Q-learning)
            max_q_next = 0.0
            if not done:
                max_q_next = max(Q[next_abstract_state].values())
            
            # G: Return ∆∞·ªõc t√≠nh (r + gamma * max(Q(s',a')))
            G = reward + GAMMA * max_q_next
            
            # C·∫≠p nh·∫≠t Q-value: Q(s,a) += alpha * (G - Q(s,a))
            Q[old_abstract_state][action_name] += ALPHA * (G - Q[old_abstract_state][action_name])
            
            # Chuy·ªÉn tr·∫°ng th√°i
            abstract_state = next_abstract_state
            total_reward += reward
        
        # --- K·∫æT TH√öC EPISODE ---
        
        is_success = env.state == env.goal and set(env.waypoints).issubset(env.visited_waypoints)
        if is_success:
            success_count += 1
            
        # Gi·∫£m Epsilon
        epsilon = max(MIN_EPSILON, epsilon * EPSILON_DECAY_RATE)
        
        # In k·∫øt qu·∫£ v√† l∆∞u model
        if (episode + 1) % 1000 == 0: 
            status = "‚úÖ DONE" if is_success else "‚ùå FAIL"
            # C·∫≠p nh·∫≠t in ·∫•n ƒë·ªÉ ph·∫£n √°nh EPSILON_DECAY_RATE m·ªõi
            print(f"Episode: {episode+1:6d} | Epsilon: {epsilon:.6f} | Steps: {env.steps:3d} | Reward: {total_reward:6.2f} | Status: {status} | Success Rate (last 1000): {success_count/1000:.2f} | States: {len(Q)}")
            success_count = 0 # Reset ƒë·∫øm th√†nh c√¥ng
        
        if (episode + 1) % 10000 == 0 or episode == EPISODES - 1:
            # L∆∞u Q-table v√† epsilon, episode
            with open(q_file, "wb") as f:
                pickle.dump({'Q': dict(Q), 'epsilon': epsilon, 'episode': episode}, f)
            print(f"üíæ ƒê√£ l∆∞u model t·∫°i episode {episode+1}. K√≠ch th∆∞·ªõc Q-table: {len(Q)}")

    end_time = datetime.now()
    print(f"\n--- Ho√†n th√†nh hu·∫•n luy·ªán {EPISODES} episodes ---")
    print(f"T·ªïng th·ªùi gian hu·∫•n luy·ªán: {end_time - start_time}")

if __name__ == "__main__":
    print("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán Q-learning (State Abstraction) v·ªõi Tham s·ªë T·ªëi ∆∞u...")
    print(f"GAMMA={GAMMA}, ALPHA={ALPHA}, EPSILON_DECAY={EPSILON_DECAY_RATE}")
    train_qlearning_with_abstraction()
